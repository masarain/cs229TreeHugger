{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nnclouds.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"7eHMLaJzF4Kf","colab":{}},"source":["# %%\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import cv2\n","import imageio\n","import pandas as pd\n","import glob, os\n","import numpy as np\n","\n","fileDir = os.getcwd()\n","# os.chdir(\"./train-jpg\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YHF8S3dIGYrO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f7d09cbc-f216-4a25-cb53-e9827cf6e5bc","executionInfo":{"status":"ok","timestamp":1575835264763,"user_tz":480,"elapsed":677,"user":{"displayName":"Jacky Lin","photoUrl":"","userId":"14749808058597004009"}}},"source":["print(os.getcwd())"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yi7WDGN0GPyg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":443},"outputId":"b204710f-d03d-4f8b-873c-1111b8532e0e","executionInfo":{"status":"error","timestamp":1575835230265,"user_tz":480,"elapsed":341,"user":{"displayName":"Jacky Lin","photoUrl":"","userId":"14749808058597004009"}}},"source":["\n","\n","# there are 40480 training examples\n","# we will allocate 39000 for training\n","# and the remaining 1480 will be for validation\n","\n","input_size = 65536 # 256^2\n","hidden_size = 20\n","hidden_size_1 = 15\n","hidden_size_2 = 10\n","hidden_size_3 = 5\n","num_classes = 1\n","learning_rate = 0.001\n","num_epochs = 5\n","\n","train_num = 1000\n","test_num = 148\n","\n","# train_num = 39000\n","# test_num = 1480\n","\n","# %% Load data--for clouds and non-clouds\n","images = []\n","\n","for file in glob.glob(\"*.jpg\"):\n","    images.append(file)\n","images = sorted(images, key=lambda filename: int(filename[6: -4])) # string splicing so that the images are in order\n","\n","train_images = []\n","test_images = []\n","\n","train_labels = []\n","test_labels = []\n","labels = pd.read_csv(\"./train_v2.csv\") # labels are whether or not image is any sort of cloudy or haze\n","\n","for i in range(train_num + test_num):\n","    tags = labels.iloc[i][\"tags\"]\n","    if i < train_num:\n","        train_images.append(imageio.imread(images[i], as_gray=True).flatten())\n","        train_labels.append(int(\"cloudy\" not in tags and \"haze\" not in tags))\n","        # train_labels.append(int(\"water\" not in tags))\n","    else:\n","        test_images.append(imageio.imread(images[i], as_gray=True).flatten())\n","        test_labels.append(int(\"cloudy\" not in tags and \"haze\" not in tags))\n","        # test_labels.append(int(\"water\" not in tags))\n"," "],"execution_count":3,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-9bc04ecee907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./train_v2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# labels are whether or not image is any sort of cloudy or haze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./train_v2.csv' does not exist: b'./train_v2.csv'"]}]},{"cell_type":"code","metadata":{"id":"Hb_FiJx_GU5z","colab_type":"code","colab":{}},"source":["       \n","\n","# %%\n","class Net(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(Net, self).__init__()\n","        \n","        # parameters\n","        \n","        # weights\n","        # self.h1 = nn.Sigmoid() # input_size, hidden_size\n","        # self.o = nn.Sigmoid() # hidden_size, num_classes\n","\n","        self.h1 = nn.Linear(input_size, hidden_size) \n","        self.h2 = nn.Linear(hidden_size, hidden_size_1)\n","        self.h3 = nn.Linear(hidden_size_1, hidden_size_2)\n","        self.h4 = nn.Linear(hidden_size_2, hidden_size_3)\n","        self.o = nn.Linear(hidden_size_3, num_classes)  \n","\n","    def forward(self, x):\n","        x = torch.sigmoid(self.h1(x))\n","        # print(\"doing x: {}\".format(x.shape))\n","        x = torch.sigmoid(self.h2(x))\n","        x = torch.sigmoid(self.h3(x))\n","        x = torch.sigmoid(self.h4(x))\n","        x = torch.sigmoid(self.o(x))\n","        return x\n","\n","# %%\n","\n","model = Net(input_size, hidden_size, num_classes) # no device configuration here\n","criterion = nn.SoftMarginLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n","# model = TheModelClass(*args, **kwargs)\n","# model.load_state_dict(torch.load(\"model.ckpt\"))\n","# model.eval()\n","# optimizer = TheOptimizerClass(*args, **kwargs)\n","\n","# checkpoint = torch.load('./model.ckpt')\n","# model.load_state_dict(checkpoint['model_state_dict'])\n","# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","# epoch = checkpoint['epoch']\n","# loss = checkpoint['loss']\n","\n","\n","total_step = len(train_images)\n","for epoch in range(num_epochs):\n","    for i, image in enumerate(train_images):  \n","\n","        image = torch.Tensor(train_images[i]).reshape(1, 65536)\n","        label = torch.Tensor([int(train_labels[i])])\n","        # label = label.long()\n","        # label = label.reshape(1,1)\n","        # label = label.squeeze()\n","        \n","        # Forward pass\n","        outputs = model(image)\n","        outputs = outputs.squeeze(0)\n","        # outputs.reshape(1,)\n","        loss = criterion(outputs, label)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","\n","\n","# %%\n","\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, image in enumerate(test_images):\n","        image = torch.Tensor(test_images[i]).reshape(1, 65536)\n","        label = torch.Tensor([int(test_labels[i])])\n","        outputs = model(image)\n","        outputs = outputs.squeeze(0)\n","        outputs = 1 if torch.sum(outputs) >= 0.5 else 0\n","        if outputs == torch.sum(label):\n","            correct += 1\n","        elif outputs == 0: \n","            print(\"#############\")\n","            print(i,outputs, torch.sum(label))\n","        # _, predicted = torch.max(outputs.data, 1)\n","        # correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of the network on the {} test images: {} %'.format(len(test_images), 100 * correct / len(test_images)))\n","\n","\n","\n","# %%\n","\n","torch.save(model.state_dict(), 'modelclouds.ckpt')\n","\n","# %%\n"],"execution_count":0,"outputs":[]}]}